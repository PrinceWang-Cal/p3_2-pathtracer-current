{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS184: Computer Graphics\n",
    "## Sping 2022\n",
    "## Assignment 3-2: Pathtracer\n",
    "\n",
    "## Prince Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "An overview of the project, your approach to and implementation for each of the parts, and what problems you encountered and how you solved them. Strive for clarity and succinctness.\n",
    "On each part, make sure to include the results described in the corresponding Deliverables section in addition to your explanation. If you failed to generate any results correctly, provide a brief explanation of why.\n",
    "The final (optional) part for the art competition is where you have the opportunity to be creative and individual, so be sure to provide a good description of what you were going for and how you implemented it.\n",
    "Clearly indicate any extra credit items you completed, and provide a thorough explanation and illustration for each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Ray Generation and Scene Intersection (20 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "- Walk through the ray generation and primitive intersection parts of the rendering pipeline.\n",
    "- Explain the triangle intersection algorithm you implemented in your own words.\n",
    "- Show images with normal shading for a few small .dae files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Walk through the ray generation and primitive intersection parts of the rendering pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the ray by sending a ray from our camera towards the scene. To do so, we first compute the set the ray origin and compute the direction of the ray pointing towards its corresponding location on the camera sensor (i.e. we transform the image coordinate to camera space and generate ray in the camera space). Then, we trasnsform it into the world space. To obtain the radiance over the pixels, we estimate the integral by averaaging multiple samples of the ray. Once we cast the rays, they continue travel until hitting an object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain the triangle intersection algorithm you implemented in your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the triangle intersection algorithm I used the Moller-Trumbore Algorithm. I first computed the five variables using the points on the triangle as well as ray direction and ray origin which can be directly accessed in the struct. Then, I compute the inverse of the determinant, and using the equation I compute t and b0,b1 and b2 (the 3 barycentric coefficients). Lastly, I check two conditions. 1. if t is bounded within r.min_t and r.max_t 2. if b0, b1 and b2 are all between 0 and 1. If both are true, then there is an intersection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/CBempty.png\" style=\"width: 500px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> </figcaption></td>\n",
    "\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/banana.png\" style=\"width: 500px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> </figcaption></td>\n",
    "\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/CBempty13.png\" style=\"width: 500px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> </figcaption></td>\n",
    "\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/CBSpheres.png\" style=\"width: 500px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> </figcaption></td>\n",
    "\n",
    "\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Bounding Volume Hierarchy (20 pts)\n",
    "\n",
    "### Requirements\n",
    "- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.\n",
    "- Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.\n",
    "- Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.\n",
    "\n",
    "####  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point:\n",
    "\n",
    "Our BVH construction algorithm is as follows:\n",
    "We first iterate over all objects we have and keep expanding the bbox that bounds all objects. Then, we count the number of objects we have, it smaller than max_leaf_size, we just reutnr the node as a leaf node. Otherwise, we will split into left node and right node.\n",
    "\n",
    "\n",
    "Our heuristic for choosing where to split is as follows:\n",
    "We first compute the mean x,y,z coordinate values of our bounding box centroids. Then, we count how many centroids are on the left side and right side of this mean value on each of the x,y,z axis. We then want to compare, how uneven it will be if we split along an axis. To do so, we compute the absolute difference between number of objects on the left and number of objects on the right for each of the x,y,z axis. The one with smallest absolute difference is the axis we want to split on, since this will create least uneveness after spliting.\n",
    "\n",
    "#### Show images with normal shading for a few large .dae files that you can only render with BVH acceleration:\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/cow.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> </figcaption></td>\n",
    "\n",
    "<td> <img src=\"./webpage_img/maxplanck.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> </figcaption></td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/CBlucy.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> </figcaption></td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "#### Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.\n",
    "\n",
    "Without using BVH acceleration, the rendering for cow.png took about 30 seconds. With BVH acceleration, I was able to render it with less than a second. For the maxplanck.png, before BVH acceleration, it took many hours to render. I left it overnight and it only finished the next morning. With BVH acceleration, it took about 2 seconds. We averaged 0.4796 million rays per second. For the CBlucy.png, I did not render it before I implement the acceleration, so I don't know how long it takes to render without BVH, but with BVH it only took a few seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Direct Illumination (20 pts)\n",
    "\n",
    "### Requirements\n",
    "- Walk through both implementations of the direct lighting function.\n",
    "- Show some images rendered with both implementations of the direct lighting function.\n",
    "- Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.\n",
    "- Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.\n",
    "\n",
    "####  Walk through both implementations of the direct lighting function:\n",
    "\n",
    "For direct lighting sampling uniformly over the hemisphere:\n",
    "we first iterate over (scene->lights.size() * ns_area_light) samples and obtrain inputs sampled from points on the hemisphere uniformly. Notice that among all these samples, many will not be coming directly from our light source. Then, we calculate the cosine of our input light, and use hit_p as ray origin and the direction in world coordinate to create a ray. With this ray, we check if we have an interesction with our BVH, and record the light coming off that intersection if there is any. We aggregate over all the samples according to the formula, and divide the aggregation by the number of sample. This gives us a good estimate of the integral.\n",
    "\n",
    "For direct lighting with importance sampling:\n",
    "This method is essentially the same as the above method, but there is a key difference. Instead of iterating over (scene->lights.size() * ns_area_light) samples, we iterate over our light source, and sample a set amount of times (unless the light source is delta_light, in which case we sample once). We then divide the samples by their pdf, and at last divide by the number of samples. So the main difference is just that we sample directly from our light source instead of uniformly over the hemisphere. \n",
    "\n",
    "#### Show some images rendered with both implementations of the direct lighting function.\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/bunny_1_1_wu.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> bunny with L=1, S=1 with uniform sampling </figcaption></td>\n",
    "\n",
    "<td> <img src=\"./webpage_img/bunny_1_1_wu_imp.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> bunny with L=1, S=1 with importance sampling </figcaption></td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/spherel_wu.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> sphere with L=1, S=1 with uniform sampling </figcaption></td>\n",
    "\n",
    "<td> <img src=\"./webpage_img/spherel_imp.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> sphere with L=1, S=1 with importance sampling </figcaption></td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "\n",
    "#### Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/spherel_l1_s1.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> sphere with L=1, S=1 with importance sampling </figcaption></td>\n",
    "\n",
    "<td> <img src=\"./webpage_img/spherel_l4_s1.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> sphere with L=4, S=1 with importance sampling </figcaption></td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/spherel_l16_s1.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> sphere with L=16, S=1 with importance sampling </figcaption></td>\n",
    "\n",
    "<td> <img src=\"./webpage_img/spherel_l64_s1.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> sphere with L=64, S=1 with importance sampling </figcaption></td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "#### Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.\n",
    "\n",
    "We notice that for the uniform sampling result, the images we get are much more noisy, whereas the importance sampling images are much clearer and also a lot brighter. The reason for this huge difference is because that when we sample light rays uniformly from the hemisphere, we don't always sample rays from the light source. Most of the time we just sample points that are darker, from non-light source regions. On the other hand, with importance sampling we are always sampling light rays from the light source, making our images 1. much brigher and 2. much less noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Global Illumination (20 pts)\n",
    "\n",
    "### Requirements\n",
    "- Walk through your implementation of the indirect lighting function.\n",
    "- Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.\n",
    "- Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)\n",
    "- For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.\n",
    "- Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.\n",
    "\n",
    "\n",
    "#### Walk through your implementation of the indirect lighting function.\n",
    "\n",
    "Our algorithm is pretty similar to the one on the lecture slide. We begin with our initial bounce, then we continuely generate new ray if we hit an object along the way. After we reached the minimum number of bounces (when it is equal to the max_ray_depth), then we will use the coinflip function to determine whether we should terminate or not (this scheme is also called Russian Roulette). This prevents infinite recursions on the bounces. We then aggregate L_out with future accumulations sent back by our next_ray (with depth = r.depth-1). However, if our new ray does not intersect with our bvh at all, we terminate early and just return L_out. Also notice, for the new rays, we set min_t as EPS_F, otherwise the algorithm will not work.\n",
    "\n",
    "#### Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/sphere_global.png\" style=\"width: 500px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> </figcaption></td>\n",
    "\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/bunny_depth3.png\" style=\"width: 500px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> </figcaption></td>\n",
    "\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "#### Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/spheres.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> only with indirect illumination </figcaption></td>\n",
    "\n",
    "<td> <img src=\"./webpage_img/spheres_indirect.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> only with direct illumination </figcaption></td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "Notice that for indirect illumination, only the light reflected of the surroundings (the walls) are projected on the spheres, giving the spheres red and blue color. On the other hand, for direct illumination, no lights reflected of the surroundings, thus the spheres does not have any color. The position of the shadow also shows the difference between direct and indirect illumination\n",
    "\n",
    "\n",
    "#### For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/bunny_s1024.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> M=0 </figcaption></td>\n",
    "\n",
    "<td> <img src=\"./webpage_img/bunny_depth1.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> M=1 </figcaption></td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/bunny_depth2.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> M=2 </figcaption></td>\n",
    "\n",
    "<td> <img src=\"./webpage_img/bunny_depth3.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> M=3 </figcaption></td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/bunny_depth100.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> M=100 </figcaption></td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "Notice that as max_ray_depth goes deeper, we can see more lighting coming from the surrounding onto the rabbit. When M=0, we see that no light is projected on to the rabbit from the walls. There is a clear difference\n",
    "\n",
    "#### Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/sphere_pixel1.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> S=1 </figcaption></td>\n",
    "\n",
    "<td> <img src=\"./webpage_img/sphere_pixel2.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> S=2 </figcaption></td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/sphere_pixel4.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> S=4 </figcaption></td>\n",
    "\n",
    "<td> <img src=\"./webpage_img/sphere_pixel8.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> S=8 </figcaption></td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/sphere_pixel16.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> S=16 </figcaption></td>\n",
    "\n",
    "<td> <img src=\"./webpage_img/sphere_pixel64.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> S=64 </figcaption></td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/sphere_pixel1024.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> S=1024 </figcaption></td>\n",
    "\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "As we can see, as we further increase the sample-per-pixel rates, the images we produced became smoother, until S-1024 which is really good rendering and very smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Adaptive Sampling (20 pts)\n",
    "\n",
    "### Requirements\n",
    "- Walk through your implementation of the adaptive sampling.\n",
    "- Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth\n",
    "\n",
    "####  Walk through your implementation of the adaptive sampling.\n",
    "\n",
    "In this question, we mainly changed the loop. First we iterate over the number of sample. For each iteration, we sample using the gridSampler, calculate the ray casted, and then use the est_radiance_global_illumination() function to get the radiance for that particular ray. Then, we keep track of s1 and s2. In each iteration we add the illuminance of the ray's radiance to s0, and add the square of the illuminance of the ray's radiance to s1. How do we know if convergence has been achieved? In each iteration we check, if a batch has been finished, then we compute the stiatistic I. If I <= maxTolerance * miu, then we break the loop, otherwise we keep sampling.\n",
    "\n",
    "#### Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth\n",
    "\n",
    "<table><tr>\n",
    "    \n",
    "<td> <img src=\"./webpage_img/sphere_q5_image.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\">  </figcaption></td>\n",
    "\n",
    "<td> <img src=\"./webpage_img/sphere_q5_rate.png\" style=\"width: 400px;\"/>\n",
    "     <figcaption align=\"middle\" style=\"font-size: 24;\"> </figcaption></td>\n",
    "     \n",
    "</tr></table>     \n",
    "\n",
    "As labelled by the difference color on the rate image, the sampling rate for each part is very different"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
